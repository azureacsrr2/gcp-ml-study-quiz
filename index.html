<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GCP ML Engineer Study Quizzer</title>
    <!-- Load Tailwind CSS from CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Custom font import for better aesthetics */
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f3f4f6; /* Light gray background */
        }
        .correct-answer-pill {
            transition: all 0.3s ease;
        }
        /* Style for options */
        .option-item {
            @apply flex items-start bg-gray-50 p-4 rounded-lg cursor-pointer hover:bg-gray-100 transition duration-150 shadow-sm;
        }
        .option-label {
            @apply w-6 h-6 flex-shrink-0 flex items-center justify-center border-2 border-gray-400 text-sm font-bold text-gray-700 rounded-full mr-4 transition duration-150;
        }
    </style>
</head>
<body class="p-4 sm:p-8 md:p-12">

    <div class="max-w-4xl mx-auto">
        <h1 class="text-3xl font-bold text-gray-800 mb-6 text-center">GCP ML Engineer Study Quizzer</h1>
        <p class="text-center text-gray-600 mb-8">Master the GCP concepts by focusing on the Category & Explanation.</p>
        
        <!-- Quiz Card Container -->
        <div id="quiz-card" class="bg-white p-6 md:p-8 rounded-xl shadow-2xl border-t-4 border-blue-600">
            
            <!-- Header and Question -->
            <div class="mb-6 pb-4 border-b">
                <div class="flex justify-between items-center mb-2">
                    <span class="text-sm font-semibold text-blue-600 uppercase tracking-wider" id="question-counter">Question 1 of 60</span>
                    <span class="px-3 py-1 bg-gray-100 text-gray-700 text-xs font-medium rounded-full" id="question-category-label">Category: Initializing...</span>
                </div>
                <h2 class="text-xl md:text-2xl font-semibold text-gray-900" id="current-question-text">Loading question...</h2>
            </div>
            
            <!-- Options List -->
            <div id="options-container" class="space-y-4 mb-8">
                <!-- Options will be injected here -->
            </div>

            <!-- Answer Section (Hidden by default) -->
            <div id="answer-section" class="mt-6 p-4 border-l-4 border-green-500 bg-green-50 rounded-lg shadow-inner hidden">
                <p class="text-lg font-bold text-green-700 mb-2">‚úÖ Correct Answer:</p>
                <div class="font-semibold text-green-800 mb-4 px-3 py-1 inline-block bg-green-200 rounded-full" id="correct-choice"></div>
                
                <p class="text-lg font-bold text-gray-700 mb-1 mt-4">üß† Explanation:</p>
                <p class="text-gray-600 text-sm" id="explanation-text">The explanation will appear here.</p>
            </div>
            
            <!-- Controls -->
            <div class="flex justify-between items-center mt-8 pt-4 border-t">
                <button onclick="navigateQuestion(-1)" id="prev-button" class="px-4 py-2 bg-gray-200 text-gray-800 rounded-lg font-medium hover:bg-gray-300 transition duration-150 disabled:opacity-50" disabled>
                    ‚Üê Previous
                </button>
                <button onclick="revealAnswer()" id="reveal-button" class="px-6 py-2 bg-blue-600 text-white rounded-lg font-semibold shadow-md hover:bg-blue-700 transition duration-150">
                    Reveal Answer
                </button>
                <button onclick="navigateQuestion(1)" id="next-button" class="px-4 py-2 bg-blue-600 text-white rounded-lg font-medium hover:bg-blue-700 transition duration-150 disabled:opacity-50" disabled>
                    Next Question ‚Üí
                </button>
            </div>

        </div>
    </div>

    <script>
        let currentQuestionIndex = 0;
        let isAnswerRevealed = false;

        // --- Question Data (60 Comprehensive Questions) ---
        const questions = [
            // --- Category 1: ML Problem Framing & Core Concepts üéØ ---
            {
                category: "ML Problem Framing & Core Concepts üéØ (Q2, Q57)",
                question: "An organization wants to make its shuttle route efficient after users confirm stops one day ahead. What approach should you take?",
                options: [
                    { id: 'A', text: 'Build a regression model to predict passenger counts at each station.' },
                    { id: 'B', text: 'Build a classification model to predict whether the shuttle should stop at each station.' },
                    { id: 'C', text: 'Define the optimal route as the shortest route passing by all confirmed stations under capacity constraints.' },
                    { id: 'D', text: 'Build a reinforcement learning model to simulate outcomes.' }
                ],
                correct: 'C',
                explanation: "Since attendance is confirmed one day in advance, this is a **route optimization problem (non-ML)**, not a prediction problem. The most efficient approach is simply calculating the shortest path among confirmed stops, likely using an exact algorithm rather than ML."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ (Q25, Q321)",
                question: "Your object detection model has low **precision** (too many false positives). How should you adjust the model's final layer threshold to increase precision?",
                options: [
                    { id: 'A', text: 'Increase the recall.' },
                    { id: 'B', text: '**Decrease the recall.**' },
                    { id: 'C', text: 'Increase the number of false positives.' },
                    { id: 'D', text: 'Decrease the probability threshold.' }
                ],
                correct: 'B',
                explanation: "Increasing precision means reducing False Positives (FP). To reduce FP, you must raise the prediction threshold, making the model more cautious. This trade-off inherently leads to a **decrease in recall** (more False Negatives)."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ (Q56, Q161)",
                question: "You are building a fraud detection model and must prioritize detection while ensuring **Precision > 50%**. Which metric combination should you optimize?",
                options: [
                    { id: 'A', text: 'Maximize the Area Under the ROC Curve (AUC ROC).' },
                    { id: 'B', text: 'Minimize Log loss.' },
                    { id: 'C', text: 'Maximize **Area Under the Precision-Recall Curve (AUC PR)**.' },
                    { id: 'D', text: 'Maximize Recall at a Precision value of 0.50.' }
                ],
                correct: 'C',
                explanation: "For highly imbalanced datasets like fraud detection, **AUC PR (Precision-Recall)** is a superior metric to AUC ROC because it focuses on the positive class. Maximizing AUC PR is the best overall optimization objective when precision matters."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ (Q69)",
                question: "You need a social media model to detect non-compliant profile photos. You must ensure the application **does not falsely accept** a non-compliant picture. Which metric to optimize?",
                options: [
                    { id: 'A', text: 'Optimize the model‚Äôs F1 score.' },
                    { id: 'B', text: 'Optimize the model‚Äôs **Recall** to minimize false negatives.' },
                    { id: 'C', text: 'Optimize the model‚Äôs Accuracy.' },
                    { id: 'D', text: 'Optimize the model‚Äôs **Precision** to minimize false positives.' }
                ],
                correct: 'D',
                explanation: "Falsely accepting a non-compliant photo means a **False Positive** (FP). To minimize FP, you must optimize for **Precision** (Precision = TP / (TP + FP))."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ (Q88)",
                question: "A bank's loan model rejected a customer. The risk department demands the **reasons that contributed to the model's decision** for this individual case.",
                options: [
                    { id: 'A', text: 'Use the correlation with target values in the data summary page.' },
                    { id: 'B', text: 'Use the global feature importance percentages in the model evaluation page.' },
                    { id: 'C', text: 'Use **local feature importance** from the predictions (e.g., Shapley values).' },
                    { id: 'D', text: 'Vary features independently to identify the threshold that changes the classification.' }
                ],
                correct: 'C',
                explanation: "Explaining an **individual prediction** requires **local feature importance (attribution)**, which calculates how each input feature contributed to that specific output. Global importance (B) only shows overall feature relevance."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ (Q327)",
                question: "To build an AI text generator that dynamically adapts to complex writing styles, you should use the most **effective** model, which is:",
                options: [
                    { id: 'A', text: 'Deploy Llama 3 from Model Garden, and use prompt engineering techniques.' },
                    { id: 'B', text: 'Fine-tune a BERT-based model from TensorFlow Hub.' },
                    { id: 'C', text: 'Fine-tune Llama 3 from Model Garden on Vertex AI Pipelines.' },
                    { id: 'D', text: 'Use the **Gemini 1.5 Flash foundational model**.' }
                ],
                correct: 'D',
                explanation: "The task complexity (dynamic style adaptation) requires a powerful, modern **Generative AI** model. Gemini 1.5 Flash is designed for complex reasoning and high-fidelity generation, making it the most effective choice."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ (Q119)",
                question: "What is the main advantage of using machine learning over a static keyword list (200,000 keywords) for an anti-spam service?",
                options: [
                    { id: 'A', text: 'Posts can be compared to the keyword list much more quickly.' },
                    { id: 'B', text: 'A much longer keyword list can be used to flag spam posts.' },
                    { id: 'C', text: 'New problematic phrases and latent semantic features can be identified in spam posts.' },
                    { id: 'D', text: 'Spam posts can be flagged using far fewer keywords.' }
                ],
                correct: 'C',
                explanation: "A static keyword list is brittle. The primary value of ML is its ability to **generalize** and identify **new patterns** (problematic phrases/semantic meaning) not explicitly listed in the original keyword dataset."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ (Q141)",
                question: "In a subscription renewal model (90% renew, 10% cancel), the model predicts cancelers at 99% accuracy but renewers at 82%. How should you interpret these results?",
                options: [
                    { id: 'A', text: 'This is not a good result because accuracy should be higher for the majority class (renewers).' },
                    { id: 'B', text: 'This is not a good result because the model is performing worse than predicting that everyone will always renew.' },
                    { id: 'C', text: 'This is a **good result** because predicting the minority class (cancelers) is usually more difficult due to less data.' },
                    { id: 'D', text: 'This is a good result because the accuracy across both groups is greater than 80%.' }
                ],
                correct: 'C',
                explanation: "Achieving high accuracy on the **minority class** (cancelers) in a highly imbalanced scenario is typically the harder challenge and a strong indicator of model success, especially when predicting the minority class is the business priority."
            },

            // --- Category 2: Data Engineering & Preprocessing üõ†Ô∏è ---
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è (Q3, Q60)",
                question: "You have extreme class imbalance (less than 1% positive examples) and the model won't converge. How should you resolve this issue?",
                options: [
                    { id: 'A', text: 'Use a convolutional neural network (CNN) with max pooling.' },
                    { id: 'B', text: 'Remove negative examples until the numbers of positive and negative examples are equal.' },
                    { id: 'C', text: 'Downsample the data with **upweighting** to create a balanced sample.' },
                    { id: 'D', text: 'Use the class distribution to generate synthetic 10% positive examples.' }
                ],
                correct: 'C',
                explanation: "A standard solution for imbalance is to **downsample the majority class** and apply **upweighting** to the minority class during training. This creates a balanced training signal without discarding all majority data."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è (Q4, Q33)",
                question: "Your PySpark ETL pipeline for structured data takes over 12 hours. To speed it up using a serverless tool and SQL syntax, what should you do?",
                options: [
                    { id: 'A', text: 'Convert PySpark to SparkSQL queries and run on Dataproc.' },
                    { id: 'B', text: 'Use Data Fusion GUI to build transformation pipelines.' },
                    { id: 'C', text: 'Ingest data into Cloud SQL, convert PySpark to SQL, and use federated queries from BigQuery.' },
                    { id: 'D', text: 'Ingest data into **BigQuery**, convert PySpark commands into **BigQuery SQL queries**, and transform data in a new table.' }
                ],
                correct: 'D',
                explanation: "The constraints demand a **serverless** tool and **SQL syntax**. Loading data into BigQuery and leveraging BigQuery's high-speed SQL execution for transformations meets these requirements, replacing the slow PySpark/Dataproc pipeline."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è (Q17, Q205)",
                question: "You are streaming files with PII to Google Cloud and must ensure the PII is not accessible to unauthorized individuals. How should you secure the pipeline using the Cloud DLP API?",
                options: [
                    { id: 'A', text: 'Stream files to BigQuery, then periodically run a bulk scan using the DLP API.' },
                    { id: 'B', text: 'Stream all files to a Sensitive bucket and secure it with restrictive IAM policies.' },
                    { id: 'C', text: 'Create three buckets: **Quarantine, Sensitive, and Non-sensitive**. Write all data to Quarantine, scan periodically with DLP API, and move data accordingly.' },
                    { id: 'D', text: 'Stream data to BigQuery. Use client-side encryption before ingestion.' }
                ],
                correct: 'C',
                explanation: "The recommended practice for handling potential PII is the **three-bucket quarantine architecture**. This ensures data is scanned and classified *before* being stored in a less restrictive location, limiting access to sensitive data during the transit and staging phases."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è (Q18, Q48)",
                question: "You are training an LTV model with time-series data from BigQuery, spanning multiple time columns. How should you split the data to ensure AutoML fits the best model?",
                options: [
                    { id: 'A', text: 'Choose an automatic data split and let AutoML handle the time signal columns.' },
                    { id: 'B', text: 'Manually combine time signal columns into an array and let AutoML interpret it.' },
                    { id: 'C', text: 'Submit the data without manual transformations, specify a **Time column**, and let AutoML split data chronologically.' },
                    { id: 'D', text: 'Manually split the data based on time, ensuring validation and testing sets contain the **most recent data**.' }
                ],
                correct: 'C',
                explanation: "For time series data, **chronological splitting** is essential to prevent data leakage (using future data to predict the past). AutoML Tables supports this natively by letting you specify the time column, reserving the most recent data for validation and testing."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è (Q120, Q127)",
                question: "A categorical feature has substantial predictive power but is missing 5% of its values. How do you handle the missing data to minimize bias?",
                options: [
                    { id: 'A', text: 'Drop feature A if more than 15% of values are missing.' },
                    { id: 'B', text: 'Replace the missing values with the feature‚Äôs mode (most frequent value).' },
                    { id: 'C', text: 'Replace the missing values with a **placeholder category** indicating a missing value.' },
                    { id: 'D', text: 'Predict the missing values using linear regression.' }
                ],
                correct: 'C',
                explanation: "When a categorical feature is missing, replacing the value with a **placeholder category** (e.g., 'MISSING') is the best method. This treats 'missingness' as its own informative category, ensuring no bias is introduced by imputation, which is critical if the missing values are non-random."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è (Q249)",
                question: "You need to read over one million images from Cloud Storage at scale for TensorFlow training, minimizing I/O bottlenecks. What should you do?",
                options: [
                    { id: 'A', text: 'Load images via Cloud Storage FUSE and use `tf.data.Dataset.from_tensor_slices`.' },
                    { id: 'B', text: 'Store image URLs in a CSV file and use `tf.data.experimental.CsvDataset`.' },
                    { id: 'C', text: 'Convert the images to **TFRecords** and store them in a Cloud Storage bucket.' },
                    { id: 'D', text: 'Use `tf.data.Dataset.list_files` on a Vertex AI managed dataset.' }
                ],
                correct: 'C',
                explanation: "**TFRecords** is the binary format optimized for efficient data reading with TensorFlow, especially for large datasets. Storing sharded TFRecords in Cloud Storage and reading with `tf.data.TFRecordDataset` minimizes data I/O overhead and serializes complex data quickly."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è (Q263)",
                question: "You need to apply MaxMin scaling and one-hot encoding on hundreds of millions of BigQuery rows for a custom TensorFlow model, minimizing effort and cost.",
                options: [
                    { id: 'A', text: 'Write a SQL query to scale the numerical features. Allow TensorFlow to perform the one-hot text encoding.' },
                    { id: 'B', text: 'Use TFX components with Dataflow to encode the text features and scale the numerical features.' },
                    { id: 'C', text: 'Use BigQuery to scale the numerical features. Feed the features into Vertex AI Training. Allow TensorFlow to perform the one-hot text encoding.' },
                    { id: 'D', text: 'Write a custom Dataflow pipeline that uses the BigQueryIO connector to ingest and process the data, writing results to Cloud Storage.' }
                ],
                correct: 'A',
                explanation: "Using **BigQuery SQL** for common transformations (like scaling lookups) is highly cost-efficient. For high-cardinality features like SKUs (one-hot encoding), handling this efficiently within the training script using TensorFlow's built-in feature column API is usually preferred, balancing effort and performance."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è (Q148)",
                question: "Your ML model uses data with PII from BigQuery. You need to **reduce the sensitivity** of the dataset before training while retaining value.",
                options: [
                    { id: 'A', text: 'Use Dataflow to ingest data and then randomize the values in each sensitive column.' },
                    { id: 'B', text: 'Use the **Cloud Data Loss Prevention (DLP) API** with Dataflow to encrypt sensitive values with **Format Preserving Encryption (FPE)**.' },
                    { id: 'C', text: 'Use the Cloud Data Loss Prevention (DLP) API to scan for sensitive data, and use Dataflow to replace all sensitive data by using the encryption algorithm AES-256 with a salt.' },
                    { id: 'D', text: 'Before training, use BigQuery to select only the columns that do not contain sensitive data.' }
                ],
                correct: 'B',
                explanation: "**Format Preserving Encryption (FPE)**, provided by the DLP API, is the best technique for de-identifying PII. FPE replaces the original value with a tokenized string of the same length and format, preserving the data's utility (cardinality/distribution) for ML modeling while protecting the underlying PII."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è (Q79)",
                question: "You need to train an NLP model on millions of examples with 100,000 unique words. You want to preprocess words individually for a recurrent neural network (RNN). What should you do?",
                options: [
                    { id: 'A', text: 'Create a hot-encoding of all 100,000 words.' },
                    { id: 'B', text: 'Identify **word embeddings** from a pre-trained model, and use the embeddings in your model.' },
                    { id: 'C', text: 'Sort the words by frequency and use the frequencies as encodings.' },
                    { id: 'D', text: 'Assign a numerical value (1 to 100,000) to each word.' }
                ],
                correct: 'B',
                explanation: "For a large vocabulary, one-hot encoding (A) leads to an unmanageably sparse and large input layer. Using **pre-trained word embeddings** (e.g., Word2Vec, GloVe) converts words into dense, meaningful vector representations, which is the standard and efficient practice for RNNs and deep learning NLP models."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è (Q43)",
                question: "To predict car sales in different cities, which features should you use to train **city-specific relationships** between car type and sales?",
                options: [
                    { id: 'A', text: 'Three individual features: binned latitude, binned longitude, and one-hot encoded car type.' },
                    { id: 'B', text: 'One feature obtained as an element-wise product between latitude, longitude, and car type.' },
                    { id: 'C', text: 'One feature obtained as an element-wise product between **binned latitude, binned longitude, and one-hot encoded car type**.' },
                    { id: 'D', text: 'Two feature crosses: the first between binned latitude and car type, and the second between binned longitude and car type.' }
                ],
                correct: 'C',
                explanation: "To capture the **interaction effect** that makes a relationship 'city-specific' (e.g., SUVs sell better in this specific location), you need a **feature cross** of the location (binned lat/long) and the car type. Combining all three provides the most specific and strongest signal for this relationship."
            },

            // --- Category 3: Model Development & Training üíª ---
            {
                category: "Model Development & Training üíª (Q13, Q109)",
                question: "During batch training of a neural network, you notice **oscillation in the loss**. How should you adjust your model to ensure it converges?",
                options: [
                    { id: 'A', text: 'Increase the size of the training batch.' },
                    { id: 'B', text: 'Increase the learning rate hyperparameter.' },
                    { id: 'C', text: '**Decrease the learning rate hyperparameter.**' },
                    { id: 'D', text: 'Apply L2 regularization parameter of 0.4.' }
                ],
                correct: 'C',
                explanation: "Loss oscillation typically means the optimizer is overshooting the minimum in the loss landscape. The standard fix is to **decrease the learning rate**, which reduces the step size taken during each update, promoting convergence."
            },
            {
                category: "Model Development & Training üíª (Q155, Q183)",
                question: "You are training a large model on a TPU, but notice that the **TPU is not reaching its full capacity** (low utilization). What should you do?",
                options: [
                    { id: 'A', text: 'Increase the number of epochs.' },
                    { id: 'B', text: 'Decrease the learning rate.' },
                    { id: 'C', text: '**Increase the batch size.**' },
                    { id: 'D', text: 'Replace the TPU with a v4-128 slice.' }
                ],
                correct: 'C',
                explanation: "TPUs are designed for massive parallel processing and perform best with large batch sizes. **Low TPU utilization** usually indicates the batch size is too small to saturate the tensor cores. Increasing the batch size is the primary optimization step."
            },
            {
                category: "Model Development & Training üíª (Q96, Q101)",
                question: "You trained a ResNet model on a TPU but are unsatisfied with memory usage and training time. You want to iterate quickly with minimal code changes. What should you do?",
                options: [
                    { id: 'A', text: 'Reduce the number of layers in the model architecture.' },
                    { id: 'B', text: 'Reduce the global batch size from 1024 to 256.' },
                    { id: 'C', text: 'Reduce the dimensions of the images used in the model.' },
                    { id: 'D', text: 'Configure your model to use **bfloat16** instead of float32.' }
                ],
                correct: 'D',
                explanation: "TPUs are natively optimized for **bfloat16** precision. Switching to bfloat16 immediately halves the memory footprint and accelerates computation with minimal impact on accuracy and minimal code changes."
            },
            {
                category: "Model Development & Training üíª (Q66, Q285)",
                question: "Your data science team needs to perform hyperparameter tuning on a PyTorch model to optimize several parameters. What is the best method on Google Cloud?",
                options: [
                    { id: 'A', text: 'Convert the model to a Keras model, and run a Keras Tuner job.' },
                    { id: 'B', text: 'Create a Kubeflow Pipelines instance, and run a hyperparameter tuning job on Katib.' },
                    { id: 'C', text: 'Run a **Vertex AI hyperparameter tuning job** using custom containers.' },
                    { id: 'D', text: 'Run multiple local training jobs on Vertex AI Workbench notebooks.' }
                ],
                correct: 'C',
                explanation: "For frameworks like PyTorch that don't have native Vertex AI support, you must containerize the training job. The Vertex AI **hyperparameter tuning service** (powered by Vizier) works seamlessly with custom containers and handles job scheduling and parallel trial management."
            },
            {
                category: "Model Development & Training üíª (Q67, Q111)",
                question: "You have a large corpus of support cases to classify into 3 categories. You need to build, test, and deploy a service quickly.",
                options: [
                    { id: 'A', text: 'Use the Cloud Natural Language API to obtain metadata.' },
                    { id: 'B', text: 'Use BigQuery ML to build and test a logistic regression model.' },
                    { id: 'C', text: 'Create a custom TensorFlow model using Google\'s BERT pre-trained model.' },
                    { id: 'D', text: 'Use **AutoML Natural Language** to build and test a classifier, and deploy the model as a REST API.' }
                ],
                correct: 'D',
                explanation: "The constraints emphasize **quick deployment** and a managed pipeline. **AutoML Natural Language** is the fastest way to build a high-accuracy custom classifier with minimal code, and it provides simple REST API deployment."
            },
            {
                category: "Model Development & Training üíª (Q71, Q87)",
                question: "You need to train a regression model (50,000 records, 20 features, target can be negative) minimizing effort and training time.",
                options: [
                    { id: 'A', text: 'Create a custom TensorFlow DNN model.' },
                    { id: 'B', text: 'Use AutoML Tables to train the model.' },
                    { id: 'C', text: 'Use **BQML XGBoost regression** to train the model.' },
                    { id: 'D', text: 'Use BQML ARIMA_PLUS forecasting model.' }
                ],
                correct: 'C',
                explanation: "The dataset is moderate size, structured, and already in BigQuery. **BigQuery ML XGBoost** is a highly efficient algorithm for tabular regression and minimizes both effort (simple SQL interface) and training time (serverless scaling)."
            },

            // --- Category 4: MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è ---
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è (Q8, Q39)",
                question: "You want to automatically run a Kubeflow Pipelines training job when new cleaned data files become available in a Cloud Storage bucket.",
                options: [
                    { id: 'A', text: 'Use App Engine to create a lightweight python client that continuously polls Cloud Storage.' },
                    { id: 'B', text: 'Use Cloud Scheduler to schedule jobs at a regular interval to check object timestamps.' },
                    { id: 'C', text: 'Configure a Cloud Storage trigger to send a message to **Pub/Sub**. Use a **Pub/Sub-triggered Cloud Function** to start the training job on a GKE cluster.' },
                    { id: 'D', text: 'Configure a Dataflow pipeline to save the files in Cloud Storage and then start the training job.' }
                ],
                correct: 'C',
                explanation: "This is the classic event-driven MLOps trigger architecture: **Cloud Storage** events fire to a **Pub/Sub** topic, which reliably triggers a **Cloud Function** (or Cloud Run service) to execute the Kubeflow Pipeline run."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è (Q19, Q217)",
                question: "You need to automate the execution of unit tests whenever code is pushed to your development branch in Cloud Source Repositories.",
                options: [
                    { id: 'A', text: 'Set up a Cloud Logging sink to a Pub/Sub topic to capture interactions.' },
                    { id: 'B', text: 'Write a script that sequentially performs the push and executes the unit tests on Cloud Run.' },
                    { id: 'C', text: 'Use **Cloud Build**, set an automated trigger to execute the unit tests when changes are pushed to the repository.' },
                    { id: 'D', text: 'Create an automated workflow in Cloud Composer that runs daily and looks for changes.' }
                ],
                correct: 'C',
                explanation: "The standard practice for **Continuous Integration (CI)** on Google Cloud is to use **Cloud Build**. Its Git triggers allow you to automatically execute testing and build steps upon every push to a specific branch."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è (Q124, Q129)",
                question: "You need an end-to-end training pipeline for a TensorFlow model (several TB of data) that includes data quality and model quality checks, minimizing development and maintenance.",
                options: [
                    { id: 'A', text: 'Create the pipeline using Kubeflow Pipelines DSL and orchestrate it using Kubeflow on GKE.' },
                    { id: 'B', text: 'Create the pipeline using **TensorFlow Extended (TFX)** and standard TFX components. Orchestrate the pipeline using **Vertex AI Pipelines**.' },
                    { id: 'C', text: 'Create the pipeline using Kubeflow Pipelines DSL and predefined Google Cloud components. Orchestrate the pipeline using Vertex AI Pipelines.' },
                    { id: 'D', text: 'Rewrite steps as an Apache Spark job and schedule execution on ephemeral Dataproc clusters.' }
                ],
                correct: 'B',
                explanation: "**TFX** is the recommended framework for ML production pipelines as it includes native, standardized components for data validation, model analysis, and quality checks. **Vertex AI Pipelines** provides the necessary managed, low-maintenance orchestration layer."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è (Q40)",
                question: "Hypertuning is taking too long, delaying downstream processes. How should you speed up the job without significantly compromising effectiveness? (Choose two)",
                options: [
                    { id: 'A', text: 'Decrease the number of parallel trials.' },
                    { id: 'B', text: 'Decrease the range of floating-point values.' },
                    { id: 'C', text: 'Set the **early stopping** parameter to TRUE.' },
                    { id: 'D', text: 'Change the search algorithm from Bayesian search to random search.' },
                    { id: 'E', text: 'Decrease the **maximum number of trials** during subsequent training phases.' }
                ],
                correct: 'C',
                explanation: "To reduce time, you must stop poor-performing trials early (**early stopping**) and limit the overall budget (**maximum number of trials**). These are direct configurations in the Vertex AI Hyperparameter Tuning service designed for this purpose."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è (Q203, Q242)",
                question: "Your team trains many models using Vertex AI Pipelines and Notebooks. You want a single system to track model parameters, metrics (per epoch), and compare performance across all versions.",
                options: [
                    { id: 'A', text: 'Vertex AI Pipelines, Vertex AI Feature Store, and Vertex AI Vizier.' },
                    { id: 'B', text: 'Vertex AI Pipelines, Vertex AI Experiments, and Vertex AI Vizier.' },
                    { id: 'C', text: 'Vertex ML Metadata, **Vertex AI Experiments**, and **Vertex AI TensorBoard**.' },
                    { id: 'D', text: 'Vertex AI Pipelines, Vertex AI Feature Store, and Vertex AI TensorBoard.' }
                ],
                correct: 'C',
                explanation: "**Vertex AI Experiments** tracks and compares metrics across runs. **Vertex AI TensorBoard** is essential for visualizing per-epoch metrics and debugging, while **Vertex ML Metadata** provides the artifact lineage."
            },

            // --- Category 5: MLOps - Serving, Monitoring & Management üí° ---
            {
                category: "MLOps - Serving, Monitoring & Management üí° (Q1, Q29, Q83)",
                question: "You need a serverless pipeline for high-throughput online inference with **computationally expensive preprocessing**. Which architecture should you use?",
                options: [
                    { id: 'A', text: 'Send requests to Pub/Sub. Use Cloud Function for preprocessing. Submit prediction request to AI Platform.' },
                    { id: 'B', text: 'Stream requests into Cloud Spanner. Query a view for preprocessing logic. Submit prediction request to AI Platform.' },
                    { id: 'C', text: 'Send requests to Pub/Sub. **Transform the data using a Dataflow job**. Submit a prediction request to AI Platform using the transformed data.' },
                    { id: 'D', text: 'Deploy two models: one trained on raw data and one trained on preprocessed data.' }
                ],
                correct: 'C',
                explanation: "For high-throughput inference requiring heavy, stateful preprocessing, **Dataflow** is the best managed tool to handle scalable stream processing and transformation before invoking the deployed model endpoint."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí° (Q14, Q30)",
                question: "Six months after deployment, a DNN model's accuracy is steadily deteriorating due to a **change in the distribution of input data**. What is the best immediate step?",
                options: [
                    { id: 'A', text: 'Perform feature selection and retrain the model with fewer features.' },
                    { id: 'B', text: 'Retrain the model using L2 regularization.' },
                    { id: 'C', text: 'Create **alerts to monitor for skew** using Vertex AI Model Monitoring, and **retrain the model**.' },
                    { id: 'D', text: 'Perform feature selection and retrain the model on a monthly basis.' }
                ],
                correct: 'C',
                explanation: "Deterioration caused by changing input data is **data drift**. The solution is continuous monitoring for this skew (using Vertex AI Model Monitoring) and triggering a retraining job based on the alert."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí° (Q46, Q77, Q189)",
                question: "You need to execute a **batch prediction** on 100 million records in a BigQuery table using a custom TensorFlow model, minimizing effort.",
                options: [
                    { id: 'A', text: 'Run a batch prediction job on Vertex AI that points to the BigQuery table.' },
                    { id: 'B', text: 'Create a Dataflow pipeline to read the data and write the predictions to BigQuery.' },
                    { id: 'C', text: '**Import the TensorFlow model with BigQuery ML, and run the ML.PREDICT function.**' },
                    { id: 'D', text: 'Export data to Cloud Storage and configure a Vertex AI batch prediction job.' }
                ],
                correct: 'C',
                explanation: "The simplest solution that minimizes effort and infrastructure setup is leveraging **BigQuery ML**. You can import the SavedModel and run inference directly on the massive table using standard SQL (`ML.PREDICT`)."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí° (Q74, Q145)",
                question: "Your DNN regressor model (serving on CPUs) has $10 \text{ ms}$ latency, but requirements demand $8 \text{ ms}$. You accept a small performance drop. What is the fastest optimization to try?",
                options: [
                    { id: 'A', text: 'Switch from CPU to GPU serving.' },
                    { id: 'B', text: 'Increase the dropout rate to 0.8 and retrain your model.' },
                    { id: 'C', text: 'Apply **quantization** to your SavedModel by reducing the floating point precision to $tf.float16$.' },
                    { id: 'D', text: 'Increase the max\_batch\_size serving parameter.' }
                ],
                correct: 'C',
                explanation: "**Quantization** (reducing precision from $float32/64$ to $float16/int8$) is a post-training optimization that significantly reduces model size and latency, with minimal effect on accuracy, making it the fastest technique to meet a tight latency target."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí° (Q163, Q339)",
                question: "A coupon model's predictions are too slow because the serving pipeline must join real-time cart data with customers' historic purchase data in **BigQuery**. How should you speed up predictions?",
                options: [
                    { id: 'A', text: 'Attach an NVIDIA P100 GPU to your deployed model‚Äôs instance.' },
                    { id: 'B', text: 'Deploy your model to more instances behind a load balancer.' },
                    { id: 'C', text: 'Use a **low latency database** (like Vertex AI Feature Store or Cloud Bigtable) for the customers‚Äô historic purchase behavior.' },
                    { id: 'D', text: 'Create a materialized view in BigQuery with the necessary data for predictions.' }
                ],
                correct: 'C',
                explanation: "The bottleneck is the **feature lookup time** from BigQuery, which is optimized for analytics, not low-latency serving. The fastest way to fix this is to migrate the critical feature data to a **low-latency feature store**."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí° (Q174, Q190)",
                question: "Your model drifts frequently, so you use Vertex AI Model Monitoring. To **reduce cost** while continuing to quickly detect drift, what should you adjust?",
                options: [
                    { id: 'A', text: 'Replace the monitoring job with a custom SQL script running on BigQuery.' },
                    { id: 'B', text: 'Increase the `monitor_interval` parameter in the ScheduleConfig of the monitoring job.' },
                    { id: 'C', text: 'Decrease the **`sample_rate` parameter** in the RandomSampleConfig of the monitoring job.' },
                    { id: 'D', text: 'Use the features and the feature attributions for monitoring.' }
                ],
                correct: 'C',
                explanation: "The cost of model monitoring scales with the amount of data analyzed. To reduce cost, you must monitor a smaller portion of the incoming traffic by **decreasing the prediction sampling rate** (sampling closer to 0 than 1)."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí° (Q328)",
                question: "A small company needs a **scalable, reliable, cost-effective** serving solution with **minimal infrastructure expertise**.",
                options: [
                    { id: 'A', text: 'Configure Compute Engine VMs to host your models.' },
                    { id: 'B', text: 'Create a Cloud Run function to deploy your models as serverless functions.' },
                    { id: 'C', text: 'Create a managed cluster on Google Kubernetes Engine (GKE), and deploy your models as containers.' },
                    { id: 'D', text: '**Deploy your models on Vertex AI endpoints.**' }
                ],
                correct: 'D',
                explanation: "**Vertex AI Endpoints** is the single managed service that provides all necessary production features (autoscaling, monitoring, reliability) with zero infrastructure management burden, making it ideal for a team with limited cloud expertise."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí° (Q329)",
                question: "LLM application has inconsistent **verbosity**. Seek a **scalable** solution to meet user expectations.",
                options: [
                    { id: 'A', text: 'Implement a **keyword-based routing layer**. If the user\'s input contains keywords, **re-prompt the language model** to summarize or describe.' },
                    { id: 'B', text: 'Ask users to provide examples of responses with the appropriate verbosity as a list of question and answer pairs. Use this dataset to perform supervised fine tuning of the foundational model. Re-evaluate the verb' }
                ],
                correct: 'A',
                explanation: "Controlling LLM behavior (like verbosity) is best done through **prompt engineering**. Implementing a scalable routing layer to modify the prompt based on user keywords is the most practical and immediate fix."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí° (Q334)",
                question: "Managing a deployed Vertex AI model and wanting to **automatically retrain** it when performance **deteriorates**.",
                options: [
                    { id: 'A', text: '**Create a Vertex AI Model Monitoring job to track the model\'s performance with production data, and trigger retraining when specific metrics drop below predefined thresholds.**' },
                    { id: 'B', text: 'Collect feedback from end users, and retrain the model based on their assessment of its performance.' },
                    { id: 'C', text: 'Configure a scheduled job to evaluate the model\'s performance on a static dataset, and retrain the model if the performance drops below predefined thresholds.' },
                    { id: 'D', text: 'Use Vertex Explainable AI to analyze feature attributions and identify potential biases in the model. Retrain when significant shifts in feature importance or biases are detected.' }
                ],
                correct: 'A',
                explanation: "The solution must be **automatic**. **Vertex AI Model Monitoring** is the native service that tracks performance against baselines and sends alerts that trigger an automated retraining pipeline when deterioration is detected."
            }
        ];
        // --- End Question Data ---

        function loadQuestion(index) {
            if (index < 0 || index >= questions.length) return;
            
            currentQuestionIndex = index;
            isAnswerRevealed = false;
            const q = questions[index];

            document.getElementById('question-counter').textContent = `Question ${index + 1} of ${questions.length}`;
            document.getElementById('question-category-label').textContent = `Category: ${q.category}`;
            document.getElementById('current-question-text').textContent = q.question;

            const optionsContainer = document.getElementById('options-container');
            optionsContainer.innerHTML = '';
            
            q.options.forEach(option => {
                const optionElement = document.createElement('div');
                optionElement.className = 'option-item';
                optionElement.innerHTML = `
                    <span class="option-label" id="option-label-${option.id}">${option.id}.</span>
                    <p class="text-gray-800 text-base flex-grow" id="option-text-${option.id}">${option.text}</p>
                `;
                optionElement.onclick = () => selectOption(option.id);
                optionsContainer.appendChild(optionElement);
            });

            document.getElementById('answer-section').classList.add('hidden');
            document.getElementById('reveal-button').classList.remove('hidden');
            
            updateNavigationButtons();
        }

        function selectOption(selectedId) {
            if (isAnswerRevealed) return;

            questions[currentQuestionIndex].options.forEach(option => {
                const element = document.getElementById(`option-label-${option.id}`).parentNode;
                element.classList.remove('bg-blue-200', 'border-blue-600');
                if (option.id === selectedId) {
                    element.classList.add('bg-blue-200', 'border-blue-600');
                }
            });
        }

        function revealAnswer() {
            isAnswerRevealed = true;
            const q = questions[currentQuestionIndex];
            
            // Mark the correct option in the list
            document.getElementById(`option-label-${q.correct}`).parentNode.classList.remove('bg-gray-50', 'bg-gray-100', 'bg-blue-200');
            document.getElementById(`option-label-${q.correct}`).parentNode.classList.add('bg-green-100', 'border-green-600', 'shadow-lg');
            document.getElementById(`option-label-${q.correct}`).classList.remove('border-gray-400');
            document.getElementById(`option-label-${q.correct}`).classList.add('bg-green-600', 'text-white');

            // Populate and show the answer section
            document.getElementById('correct-choice').textContent = `Choice ${q.correct}`;
            document.getElementById('explanation-text').textContent = q.explanation;
            document.getElementById('answer-section').classList.remove('hidden');
            document.getElementById('reveal-button').classList.add('hidden');
        }

        function navigateQuestion(direction) {
            // If moving forward and the answer hasn't been revealed, prompt the user.
            if (direction === 1 && currentQuestionIndex < questions.length - 1 && !isAnswerRevealed) {
                if (!confirm("Are you sure you want to move to the next question without revealing the answer? This is active learning!")) {
                    return;
                }
            }
            loadQuestion(currentQuestionIndex + direction);
        }

        function updateNavigationButtons() {
            document.getElementById('prev-button').disabled = currentQuestionIndex === 0;
            document.getElementById('next-button').disabled = currentQuestionIndex === questions.length - 1;
        }

        // Initialize the app
        window.onload = () => loadQuestion(0);
    </script>
</body>
</html>
