<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GCP ML Engineer Study Quizzer</title>
    <!-- Load Tailwind CSS from CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Custom font import for better aesthetics */
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f3f4f6; /* Light gray background */
        }
        .correct-answer-pill {
            transition: all 0.3s ease;
        }
        /* Style for options */
        .option-item {
            @apply flex items-start bg-gray-50 p-4 rounded-lg cursor-pointer hover:bg-gray-100 transition duration-150 shadow-sm;
        }
        .option-label {
            @apply w-6 h-6 flex-shrink-0 flex items-center justify-center border-2 border-gray-400 text-sm font-bold text-gray-700 rounded-full mr-4 transition duration-150;
        }
    </style>
</head>
<body class="p-4 sm:p-8 md:p-12">

    <div class="max-w-4xl mx-auto">
        <h1 class="text-3xl font-bold text-gray-800 mb-6 text-center">GCP ML Engineer Study Quizzer</h1>
        <p class="text-center text-gray-600 mb-8">Master the GCP concepts by focusing on the Category & Explanation.</p>
        
        <!-- Quiz Card Container -->
        <div id="quiz-card" class="bg-white p-6 md:p-8 rounded-xl shadow-2xl border-t-4 border-blue-600">
            
            <!-- Header and Question -->
            <div class="mb-6 pb-4 border-b">
                <div class="flex justify-between items-center mb-2">
                    <span class="text-sm font-semibold text-blue-600 uppercase tracking-wider" id="question-counter">Question 1 of 100</span>
                    <span class="px-3 py-1 bg-gray-100 text-gray-700 text-xs font-medium rounded-full" id="question-category-label">Category: Initializing...</span>
                </div>
                <h2 class="text-xl md:text-2xl font-semibold text-gray-900" id="current-question-text">Loading question...</h2>
            </div>
            
            <!-- Options List -->
            <div id="options-container" class="space-y-4 mb-8">
                <!-- Options will be injected here -->
            </div>

            <!-- Answer Section (Hidden by default) -->
            <div id="answer-section" class="mt-6 p-4 border-l-4 border-green-500 bg-green-50 rounded-lg shadow-inner hidden">
                <p class="text-lg font-bold text-green-700 mb-2">‚úÖ Correct Answer:</p>
                <div class="font-semibold text-green-800 mb-4 px-3 py-1 inline-block bg-green-200 rounded-full" id="correct-choice"></div>
                
                <p class="text-lg font-bold text-gray-700 mb-1 mt-4">üß† Explanation:</p>
                <p class="text-gray-600 text-sm" id="explanation-text">The explanation will appear here.</p>
            </div>
            
            <!-- Controls -->
            <div class="flex justify-between items-center mt-8 pt-4 border-t">
                <button onclick="navigateQuestion(-1)" id="prev-button" class="px-4 py-2 bg-gray-200 text-gray-800 rounded-lg font-medium hover:bg-gray-300 transition duration-150 disabled:opacity-50" disabled>
                    ‚Üê Previous
                </button>
                <button onclick="revealAnswer()" id="reveal-button" class="px-6 py-2 bg-blue-600 text-white rounded-lg font-semibold shadow-md hover:bg-blue-700 transition duration-150">
                    Reveal Answer
                </button>
                <button onclick="navigateQuestion(1)" id="next-button" class="px-4 py-2 bg-blue-600 text-white rounded-lg font-medium hover:bg-blue-700 transition duration-150 disabled:opacity-50" disabled>
                    Next Question ‚Üí
                </button>
            </div>

        </div>
    </div>

    <script>
        let currentQuestionIndex = 0;
        let isAnswerRevealed = false;

        // --- Question Data (100 Comprehensive Questions across all categories) ---
        const questions = [
            // --- Category 1: ML Problem Framing & Core Concepts üéØ (Q2, Q57, Q69, Q88, Q141, Q327) ---
            {
                category: "ML Problem Framing & Core Concepts üéØ",
                question: "An organization wants to make its shuttle route efficient after users confirm stops one day ahead. What approach should you take?",
                options: [
                    { id: 'A', text: 'Build a regression model to predict passenger counts at each station.' },
                    { id: 'B', text: 'Build a classification model to predict whether the shuttle should stop at each station.' },
                    { id: 'C', text: 'Define the optimal route as the shortest route passing by all confirmed stations under capacity constraints.' },
                    { id: 'D', text: 'Build a reinforcement learning model to simulate outcomes.' }
                ],
                correct: 'C',
                explanation: "Since attendance is confirmed, this is a **route optimization problem (non-ML)**. The efficient solution is calculating the shortest path among confirmed stops, not predicting attendance."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ",
                question: "You need a model for non-compliant photos that ensures the app **does not falsely accept** a non-compliant picture (minimizing False Positives). Which metric to optimize?",
                options: [
                    { id: 'A', text: 'Falsely accepting a non-compliant photo is a False Negative. Optimize Recall.' },
                    { id: 'B', text: 'Optimize the model‚Äôs F1 score.' },
                    { id: 'C', text: 'Optimize the model‚Äôs Accuracy.' },
                    { id: 'D', text: 'Falsely accepting a non-compliant photo is a **False Positive**. Optimize **Precision**.' }
                ],
                correct: 'D',
                explanation: "To minimize False Positives (FPs), you must optimize **Precision** (TP / (TP + FP)). This makes the model highly cautious about predicting compliance."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ",
                question: "In a subscription renewal model (90% renew, 10% cancel), the model predicts cancelers at 99% accuracy but renewers at 82%. How should you interpret these results?",
                options: [
                    { id: 'A', text: 'This is not a good result because accuracy should be higher for the majority class (renewers).' },
                    { id: 'B', text: 'This model performs worse than predicting that everyone will always renew.' },
                    { id: 'C', text: 'This is a **good result** because predicting the minority class (cancelers) is usually the hard, high-value task.' },
                    { id: 'D', text: 'The high accuracy on the minority class indicates data leakage.' }
                ],
                correct: 'C',
                explanation: "Achieving high accuracy on the **minority class** (cancelers) in a highly imbalanced scenario is typically the harder challenge and a strong indicator of model success, even if majority class accuracy dips."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ",
                question: "A model rejected a customer's loan. The risk department demands the **reasons** for this decision for **this individual case**.",
                options: [
                    { id: 'A', text: 'Use the global feature importance from the model evaluation page.' },
                    { id: 'B', text: 'Use **local feature importance** from the predictions (e.g., Shapley values).' },
                    { id: 'C', text: 'Check the correlation with target values in the data summary.' },
                    { id: 'D', text: 'Perform a secondary validation on a held-out dataset.' }
                ],
                correct: 'B',
                explanation: "Explaining an **individual prediction** requires **local feature attribution**, which calculates how each input feature contributed to that single output. Global importance is insufficient."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ",
                question: "To build an AI text generator that dynamically adapts to complex writing styles, the most **effective** model is:",
                options: [
                    { id: 'A', text: 'Fine-tune a BERT-based classification model.' },
                    { id: 'B', text: 'Deploy Llama 3 from Model Garden, and use basic prompt engineering.' },
                    { id: 'C', text: 'Use a simple RNN/LSTM architecture trained on labeled style data.' },
                    { id: 'D', text: 'Use the **Gemini 1.5 Flash foundational model**.' }
                ],
                correct: 'D',
                explanation: "Complex, dynamic style adaptation and high-fidelity generation tasks are best handled by a powerful, modern **Generative AI** or Large Language Model (LLM) like Gemini 1.5 Flash."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ",
                question: "You need a fraudulent transaction model that must prioritize detection while ensuring **Precision > 50%**. Which objective should you optimize?",
                options: [
                    { id: 'A', text: 'Maximize the Area Under the ROC Curve (AUC ROC).' },
                    { id: 'B', text: 'Minimize Log loss.' },
                    { id: 'C', text: 'Maximize **Area Under the Precision-Recall Curve (AUC PR)**.' },
                    { id: 'D', text: 'Maximize Recall at a fixed threshold.' }
                ],
                correct: 'C',
                explanation: "For highly imbalanced data (like fraud), **AUC PR** is superior to AUC ROC because it focuses exclusively on the positive class and penalizes False Positives, making it the best objective when precision matters."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ",
                question: "What is the primary factor you must consider when building an insurance approval model due to its regulatory nature?",
                options: [
                    { id: 'A', text: 'Differential privacy and federated learning.' },
                    { id: 'B', text: 'Redaction and low latency.' },
                    { id: 'C', text: '**Traceability, reproducibility, and explainability.**' },
                    { id: 'D', text: 'Model complexity and high F1-score.' }
                ],
                correct: 'C',
                explanation: "In regulated industries like insurance, legal compliance requires models to demonstrate clear **traceability** of data and code, **reproducible** outcomes, and **explainability** for adverse decisions."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ",
                question: "What is the main advantage of using ML over a static keyword list (200k entries) for an anti-spam service?",
                options: [
                    { id: 'A', text: 'Posts can be compared to the keyword list much more quickly.' },
                    { id: 'B', text: 'A much longer keyword list can be used to flag spam posts.' },
                    { id: 'C', text: '**New problematic phrases and latent semantic features can be identified in spam posts.**' },
                    { id: 'D', text: 'Spam posts can be flagged using far fewer keywords.' }
                ],
                correct: 'C',
                explanation: "The key value of ML is its ability to **generalize** and identify **new patterns** or subtle semantic features that are not explicitly listed in the original, brittle keyword list."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ",
                question: "You trained a deep learning model. The model has low loss on the training data, but performs poorly on the validation data. What is the root problem?",
                options: [
                    { id: 'A', text: 'High Bias (Underfitting).' },
                    { id: 'B', text: '**High Variance (Overfitting).**' },
                    { id: 'C', text: 'Incorrect batch size.' },
                    { id: 'D', text: 'Vanishing Gradient problem.' }
                ],
                correct: 'B',
                explanation: "Low loss on training data indicates the model learned the training set well. Poor performance on unseen validation data means the model learned the noise, which is the definition of **Overfitting** (High Variance)."
            },
            {
                category: "ML Problem Framing & Core Concepts üéØ",
                question: "You need a recommendation model to suggest articles similar to the one a user is currently reading. Which approach should you use?",
                options: [
                    { id: 'A', text: 'Build a collaborative filtering system based on past user behavior.' },
                    { id: 'B', text: 'Build a logistic regression model for each user.' },
                    { id: 'C', text: '**Encode all articles into vectors using Word2Vec, and build a model that returns articles based on vector similarity (Content-based filtering).**' },
                    { id: 'D', text: 'Manually label articles and train an SVM classifier.' }
                ],
                correct: 'C',
                explanation: "Suggesting items 'similar to the one they are currently reading' is a **Content-Based Filtering** task. This requires using embeddings (like Word2Vec) to represent the articles as vectors and finding nearest neighbors."
            },

            // --- Category 2: Data Engineering & Preprocessing üõ†Ô∏è (Q3, Q4, Q17, Q48, Q79, Q127, Q148, Q249) ---
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è",
                question: "You have extreme class imbalance (less than 1% positive examples) and the model won't converge. How should you resolve this issue?",
                options: [
                    { id: 'A', text: 'Use a convolutional neural network (CNN) with max pooling.' },
                    { id: 'B', text: 'Remove negative examples until the numbers of positive and negative examples are equal.' },
                    { id: 'C', text: '**Downsample the data with upweighting to create a balanced sample.**' },
                    { id: 'D', text: 'Use the class distribution to generate synthetic 10% positive examples.' }
                ],
                correct: 'C',
                explanation: "A standard solution for imbalance is to **downsample the majority class** and apply **upweighting** to the minority class during training. This creates a balanced training signal without discarding all majority data."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è",
                question: "Your PySpark ETL pipeline takes over 12 hours. To speed it up using a **serverless tool and SQL syntax**, what should you do?",
                options: [
                    { id: 'A', text: 'Convert PySpark to SparkSQL queries and run on Dataproc.' },
                    { id: 'B', text: 'Use Data Fusion GUI to build transformation pipelines.' },
                    { id: 'C', text: 'Ingest data into Cloud SQL, convert PySpark to SQL, and use federated queries from BigQuery.' },
                    { id: 'D', text: 'Ingest data into **BigQuery**, convert PySpark commands into **BigQuery SQL queries**, and transform data in a new table.' }
                ],
                correct: 'D',
                explanation: "BigQuery offers a high-speed, **serverless** engine using standard **SQL syntax**. Migrating ETL steps to BigQuery SQL drastically cuts runtime and complexity compared to managing Spark clusters."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è",
                question: "You are streaming files with PII to Google Cloud. How should you secure the pipeline using the Cloud DLP API?",
                options: [
                    { id: 'A', text: 'Stream all files to a Sensitive bucket and secure it with restrictive IAM policies.' },
                    { id: 'B', text: 'Stream files to BigQuery, then periodically run a bulk scan using the DLP API.' },
                    { id: 'C', text: 'Create three buckets: **Quarantine, Sensitive, and Non-sensitive**. Write all data to Quarantine, scan periodically with DLP API, and move data accordingly.' },
                    { id: 'D', text: 'Stream data to BigQuery. Use client-side encryption before ingestion.' }
                ],
                correct: 'C',
                explanation: "The recommended practice for PII is the **three-bucket quarantine architecture** to scan and classify data immediately upon arrival, preventing unauthorized access before the data is stored permanently."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è",
                question: "You are training a model with time-series data. How should you split the data to ensure **Autoscaled training** avoids **data leakage**?",
                options: [
                    { id: 'A', text: 'Choose an automatic data split and let AutoML handle the time signal columns.' },
                    { id: 'B', text: 'Manually combine time signal columns into an array and let AutoML interpret it.' },
                    { id: 'C', text: 'Submit the data without manual transformations, specify a **Time column**, and let AutoML split data chronologically.' },
                    { id: 'D', text: 'Manually split the data randomly 80/10/10.' }
                ],
                correct: 'C',
                explanation: "For time series data, using the **chronological split** feature and specifying a **Time column** is the only way to guarantee that the validation and test sets only contain data from *after* the training set, correctly preventing leakage."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è",
                question: "A categorical feature has substantial predictive power but is missing 5% of its values. How do you handle the missing data to minimize bias?",
                options: [
                    { id: 'A', text: 'Drop the rows with missing values.' },
                    { id: 'B', text: 'Replace the missing values with the feature‚Äôs mode.' },
                    { id: 'C', text: 'Replace the missing values with a **placeholder category** indicating a missing value.' },
                    { id: 'D', text: 'Predict the missing values using linear regression.' }
                ],
                correct: 'C',
                explanation: "For categorical data, replacing nulls with a dedicated **placeholder category** (e.g., 'UNKNOWN' or 'MISSING') is the safest method, as it preserves the information that the value was missing without introducing imputation bias."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è",
                question: "You need to read over one million images from Cloud Storage at scale for TensorFlow training, minimizing I/O bottlenecks. What is the recommended format?",
                options: [
                    { id: 'A', text: 'Store image URLs in a CSV file.' },
                    { id: 'B', text: 'Load images via Cloud Storage FUSE.' },
                    { id: 'C', text: '**Convert the images to TFRecords and store them in a Cloud Storage bucket.**' },
                    { id: 'D', text: 'Use JPEG compression level 95.' }
                ],
                correct: 'C',
                explanation: "**TFRecords** is the binary format specifically optimized for TensorFlow's internal I/O model. Sharding and storing them in Cloud Storage ensures fast, scalable data loading for distributed training."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è",
                question: "How should you reduce the sensitivity of PII data in BigQuery before training, while retaining value and utility?",
                options: [
                    { id: 'A', text: 'Use Dataflow to randomize the values in each sensitive column.' },
                    { id: 'B', text: 'Remove all sensitive data fields entirely.' },
                    { id: 'C', text: 'Use the **Cloud Data Loss Prevention (DLP) API** with Dataflow to encrypt sensitive values with **Format Preserving Encryption (FPE)**.' },
                    { id: 'D', text: 'Create an authorized view in BigQuery that restricts access to the sensitive data.' }
                ],
                correct: 'C',
                explanation: "**Format Preserving Encryption (FPE)** is the technique that tokenizes PII, allowing the model to train on the encrypted, non-sensitive representation (which maintains cardinality and length) while protecting the original data."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è",
                question: "To train an NLP model on millions of examples with 100,000 unique words, how should you preprocess words for a recurrent neural network (RNN)?",
                options: [
                    { id: 'A', text: 'Create a hot-encoding of all 100,000 words.' },
                    { id: 'B', text: 'Sort the words by frequency and use the frequencies as encodings.' },
                    { id: 'C', text: '**Identify word embeddings from a pre-trained model, and use the embeddings in your model.**' },
                    { id: 'D', text: 'Assign a numerical value (1 to 100,000) to each word.' }
                ],
                correct: 'C',
                explanation: "For large vocabularies, **word embeddings** convert sparse textual input into dense, numerically meaningful vectors, which is the standard and efficient practice for deep learning models like RNNs."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è",
                question: "Your team needs to ensure that the preprocessing logic (min-max scaling, one-hot encoding) is applied consistently between batch training and real-time inference.",
                options: [
                    { id: 'A', text: 'Deploy the training transformations in a separate Cloud Function and call it during serving.' },
                    { id: 'B', text: 'Perform data validation to ensure the input data format is the same.' },
                    { id: 'C', text: '**Refactor the transformation code from the batch data pipeline so that it can be used outside of the pipeline (e.g., in the model endpoint\'s code).**' },
                    { id: 'D', text: 'Batch the real-time requests using a time window and process them via a Dataflow pipeline.' }
                ],
                correct: 'C',
                explanation: "The Training-Serving Skew mitigation best practice is to **bundle the transformation logic** directly into the serving model or runtime environment (e.g., in a Custom Prediction Routine or TFX model) to ensure consistency."
            },
            {
                category: "Data Engineering & Preprocessing üõ†Ô∏è",
                question: "Which data transformation technique should you use to remove **non-informative features** from a **linear model**?",
                options: [
                    { id: 'A', text: 'Principal Component Analysis (PCA).' },
                    { id: 'B', text: 'Iterative dropout technique.' },
                    { id: 'C', text: '**Use L1 regularization (Lasso) to reduce the coefficients of uninformative features to 0.**' },
                    { id: 'D', text: 'Use L2 regularization (Ridge) to shrink the coefficients.' }
                ],
                correct: 'C',
                explanation: "In linear models, **L1 regularization (Lasso)** is the only technique that forces the coefficients of less important features to exactly zero, effectively performing feature selection and removal directly within the model training process."
            },

            // --- Category 3: Model Development & Training üíª (Q7, Q13, Q66, Q71, Q96, Q109, Q183, Q285, Q338) ---
            {
                category: "Model Development & Training üíª",
                question: "You need a no-code solution for classification workflows over BigQuery data, covering exploratory analysis, feature selection, training, and serving.",
                options: [
                    { id: 'A', text: 'Run a BigQuery ML task to perform logistic regression.' },
                    { id: 'B', text: 'Use AI Platform Notebooks with pandas library.' },
                    { id: 'C', text: '**Configure AutoML Tables to perform the classification task.**' },
                    { id: 'D', text: 'Use Vertex AI to run a custom classification job for hyperparameter tuning.' }
                ],
                correct: 'C',
                explanation: "The constraint for a **no-code** solution that covers the full lifecycle (EDA, feature engineering, model selection, tuning, and serving) on tabular data is uniquely met by **AutoML Tables**."
            },
            {
                category: "Model Development & Training üíª",
                question: "Your PyTorch model needs **hyperparameter tuning**. What is the most integrated method on Google Cloud?",
                options: [
                    { id: 'A', text: 'Convert the model to a Keras model, and run a Keras Tuner job.' },
                    { id: 'B', text: 'Run a custom training job on Vertex AI and manually change parameters between runs.' },
                    { id: 'C', text: 'Run a **Vertex AI hyperparameter tuning job** using custom containers.' },
                    { id: 'D', text: 'Create a Kubeflow Pipelines instance, and run a hyperparameter tuning job on Katib.' }
                ],
                correct: 'C',
                explanation: "The managed **Vertex AI hyperparameter tuning service** (powered by Vizier) is the integrated solution for efficient tuning. It works with non-TensorFlow models (like PyTorch) via **custom containers**."
            },
            {
                category: "Model Development & Training üíª",
                question: "You are training a large model on a TPU, but notice **low utilization**. What is the primary optimization step?",
                options: [
                    { id: 'A', text: 'Decrease the learning rate.' },
                    { id: 'B', text: '**Increase the batch size.**' },
                    { id: 'C', text: 'Apply stochastic gradient descent (SGD).' },
                    { id: 'D', text: 'Reduce the dimensions of the input tensor.' }
                ],
                correct: 'B',
                explanation: "**Low TPU utilization** almost always indicates the batch size is too small to saturate the massive number of parallel cores. Increasing the batch size maximizes hardware efficiency."
            },
            {
                category: "Model Development & Training üíª",
                question: "You trained a ResNet model on a TPU but are unsatisfied with memory usage and training time. You want to iterate quickly with minimal code changes. What should you do?",
                options: [
                    { id: 'A', text: 'Reduce the number of layers in the model architecture.' },
                    { id: 'B', text: 'Reduce the global batch size from 1024 to 256.' },
                    { id: 'C', text: 'Reduce the dimensions of the images used in the model.' },
                    { id: 'D', text: '**Configure your model to use bfloat16 instead of float32.**' }
                ],
                correct: 'D',
                explanation: "Switching to **bfloat16** precision is the most effective way to immediately halve the memory footprint and accelerate computation on TPUs with minimal code changes and little impact on accuracy."
            },
            {
                category: "Model Development & Training üíª",
                question: "You observe **oscillation in the loss** during neural network training. How should you adjust your model to promote convergence?",
                options: [
                    { id: 'A', text: 'Increase the size of the training batch.' },
                    { id: 'B', text: 'Increase the learning rate hyperparameter.' },
                    { id: 'C', text: '**Decrease the learning rate hyperparameter.**' },
                    { id: 'D', text: 'Apply L2 regularization parameter of 0.4.' }
                ],
                correct: 'C',
                explanation: "Loss oscillation typically means the learning rate is too high, causing the optimizer to overshoot the minimum. **Decreasing the learning rate** reduces the step size, smoothing the convergence path."
            },
            {
                category: "Model Development & Training üíª",
                question: "You need to train a regression model (50,000 records) on BigQuery data, minimizing effort and training time.",
                options: [
                    { id: 'A', text: 'Create a custom TensorFlow DNN model.' },
                    { id: 'B', text: 'Use AutoML Tables to train the model.' },
                    { id: 'C', text: '**Use BQML XGBoost regression to train the model.**' },
                    { id: 'D', text: 'Use BQML ARIMA_PLUS forecasting model.' }
                ],
                correct: 'C',
                explanation: "For tabular regression on data already in BigQuery, **BigQuery ML (BQML) XGBoost** is the lowest effort and highly efficient method due to its serverless nature and simple SQL interface."
            },
            {
                category: "Model Development & Training üíª",
                question: "You need to train a Transformer model that includes custom TensorFlow operations and expects weeks of training time. Which architecture minimizes time and compute costs?",
                options: [
                    { id: 'A', text: 'Implement 8 workers of a2-megagpu-16g machines using MultiWorkerMirroredStrategy.' },
                    { id: 'B', text: 'Implement 16 workers of c2d-highcpu-32 machines using MirroredStrategy.' },
                    { id: 'C', text: 'Implement a **TPU Pod slice** with the most powerful **v4** accelerator type using **TPUStrategy**.' },
                    { id: 'D', text: 'Implement a TPU Pod slice with the oldest v2 accelerator type.' }
                ],
                correct: 'C',
                explanation: "For extremely large, long-running models, **TPU Pod slices** (especially modern versions like v4) combined with **`tf.distribute.TPUStrategy`** offer the highest density of processing power, minimizing both training time and cost (especially if using preemptible TPUs)."
            },
            {
                category: "Model Development & Training üíª",
                question: "You developed a deep learning model. Training loss is low, but validation loss is stagnant. What debugging step should you take first?",
                options: [
                    { id: 'A', text: '**Verify that your model can obtain a low loss on a small subset of the dataset (Overfitting a small batch).**' },
                    { id: 'B', text: 'Add handcrafted features to inject domain knowledge.' },
                    { id: 'C', text: 'Use the Vertex AI hyperparameter tuning service to identify a better learning rate.' },
                    { id: 'D', text: 'Use hardware accelerators and train your model for more epochs.' }
                ],
                correct: 'A',
                explanation: "The first step in deep learning debugging is verifying the network's capacity to learn. If the model cannot overfit a small, simple batch of data, there is a fundamental bug in the model architecture or implementation."
            },
            {
                category: "Model Development & Training üíª",
                question: "You need to build classification workflows over structured BigQuery data without writing code (EDA, feature selection, training, tuning, serving).",
                options: [
                    { id: 'A', text: 'Run a logistic regression job on BigQuery ML.' },
                    { id: 'B', text: 'Train a TensorFlow model on Vertex AI.' },
                    { id: 'C', text: 'Use scikit-learn in Vertex AI Workbench with pandas library.' },
                    { id: 'D', text: '**Train a classification Vertex AutoML model.**' }
                ],
                correct: 'D',
                explanation: "The constraint for a **no-code** solution that covers the full lifecycle (EDA, feature engineering, and hyperparameter tuning) is specifically met by **Vertex AutoML**."
            },
            {
                category: "Model Development & Training üíª",
                question: "You need to train an object detection model on 3 million X-ray images ($2 \text{ GB}$ each) on a Compute Engine instance with 1 P100 GPU. Training is taking a very long time. What is the most effective solution?",
                options: [
                    { id: 'A', text: 'Increase the instance memory to $512 \text{ GB}$ and increase the batch size.' },
                    { id: 'B', text: 'Enable early stopping in your Vertex AI Training job.' },
                    { id: 'C', text: '**Replace the NVIDIA P100 GPU with a v3-32 TPU in the training job.**' },
                    { id: 'D', text: 'Use the tf.distribute.Strategy API and run a distributed training job on multiple P100s.' }
                ],
                correct: 'C',
                explanation: "For massive-scale workloads (3 million $2 \text{ GB}$ images), the sheer throughput required far exceeds that of even multiple P100 GPUs. Switching to a **Cloud TPU** (like the v3-32 slice) is the most effective way to achieve the necessary speed and parallelism for large models."
            },

            // --- Category 4: MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è (Q8, Q19, Q40, Q124, Q203, Q217, Q324, Q330, Q335) ---
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è",
                question: "How should you automate the execution of unit tests whenever code is pushed to your development branch in Cloud Source Repositories?",
                options: [
                    { id: 'A', text: 'Set up a Cloud Logging sink to a Pub/Sub topic to capture interactions.' },
                    { id: 'B', text: 'Write a script that sequentially performs the push and executes the unit tests on Cloud Run.' },
                    { id: 'C', text: '**Use Cloud Build, set an automated trigger to execute the unit tests when changes are pushed to the repository.**' },
                    { id: 'D', text: 'Create an automated workflow in Cloud Composer that runs daily and looks for changes.' }
                ],
                correct: 'C',
                explanation: "**Cloud Build** is the dedicated service for Continuous Integration (CI). Its native Git triggers automatically start defined build and test steps upon repository pushes."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è",
                question: "You need a scalable, automated pipeline (TensorFlow, several TB of data) that includes **data quality** and **model quality checks**, minimizing maintenance.",
                options: [
                    { id: 'A', text: 'Create the pipeline using Kubeflow Pipelines DSL and orchestrate it using Kubeflow on GKE.' },
                    { id: 'B', text: 'Create the pipeline using **TensorFlow Extended (TFX)** and standard TFX components. Orchestrate the pipeline using **Vertex AI Pipelines**.' },
                    { id: 'C', text: 'Create the pipeline using Kubeflow Pipelines DSL and predefined Google Cloud components. Orchestrate the pipeline using Vertex AI Pipelines.' },
                    { id: 'D', text: 'Rewrite steps as an Apache Spark job and schedule execution on ephemeral Dataproc clusters.' }
                ],
                correct: 'B',
                explanation: "**TFX** is the standardized framework with pre-built components for data and model validation. Orchestrating it with the managed service, **Vertex AI Pipelines**, minimizes maintenance effort."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è",
                question: "Hypertuning is taking too long. How should you speed up the job without significantly compromising effectiveness? (Choose two)",
                options: [
                    { id: 'A', text: 'Decrease the number of parallel trials.' },
                    { id: 'B', text: 'Decrease the range of floating-point values.' },
                    { id: 'C', text: '**Set the early stopping parameter to TRUE.**' },
                    { id: 'D', text: 'Change the search algorithm from Bayesian search to random search.' },
                    { id: 'E', text: '**Decrease the maximum number of trials** during subsequent training phases.' }
                ],
                correct: 'C',
                explanation: "To reduce time, you must stop poor-performing trials early (**early stopping**) and limit the overall budget (**maximum number of trials**). This directly speeds up the job time in the Vertex AI Hyperparameter Tuning service."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è",
                question: "You need a system to track model parameters/metrics (per epoch) and compare performance across multiple versions created via Pipelines and Notebooks.",
                options: [
                    { id: 'A', text: 'Vertex AI Pipelines, Vertex AI Feature Store, and Vertex AI Vizier.' },
                    { id: 'B', text: 'Vertex AI Pipelines, Vertex AI Experiments, and Vertex AI Vizier.' },
                    { id: 'C', text: '**Vertex ML Metadata, Vertex AI Experiments, and Vertex AI TensorBoard.**' },
                    { id: 'D', text: 'Vertex AI Pipelines, Vertex AI Feature Store, and Vertex AI TensorBoard.' }
                ],
                correct: 'C',
                explanation: "**Vertex AI Experiments** tracks comparisons, **Vertex AI TensorBoard** visualizes per-epoch metrics, and **Vertex ML Metadata** provides the artifact lineage, making this the complete tracking solution."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è",
                question: "How should you operationalize a model from a notebook to a **reliable, repeatable, weekly** training process that tracks **versioning and lineage**?",
                options: [
                    { id: 'A', text: '1. Create an instance of the CustomTrainingJob class with the Vertex AI SDK. 2. Using the Notebooks API, create a scheduled execution to run the training code weekly.' },
                    { id: 'B', text: '1. Create an instance of the CustomJob class with the Vertex AI SDK. 2. Use the Metadata API to register your model as a model artifact. 3. Using the Notebooks API, create a scheduled execution to run the training code weekly.' },
                    { id: 'C', text: '1. **Create a managed pipeline in Vertex AI Pipelines and schedule it to run weekly.**' }
                ],
                correct: 'C',
                explanation: "**Vertex AI Pipelines** is the dedicated, managed framework for creating a repeatable, scheduled MLOps process that automatically tracks versioning and lineage through Vertex ML Metadata integration."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è",
                question: "To track and compare a new model version and incorporate **dataset versioning** using Vertex AI, which services should you use?",
                options: [
                    { id: 'A', text: 'Vertex AI TensorBoard and Data Catalog.' },
                    { id: 'B', text: 'Vertex AI Model Monitoring and Vertex AI Training.' },
                    { id: 'C', text: '**Vertex AI Experiments and Vertex ML Metadata.**' },
                    { id: 'D', text: 'Vertex AI Experiments and Vertex AI managed datasets.' }
                ],
                correct: 'C',
                explanation: "**Vertex AI Experiments** organizes comparisons. **Vertex ML Metadata** is the fundamental tool for recording model lineage and the relationship with the **dataset version** used for training."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è",
                question: "You want to automatically run a Kubeflow Pipelines training job when new data files become available in a Cloud Storage bucket.",
                options: [
                    { id: 'A', text: 'Use App Engine to create a lightweight python client that continuously polls Cloud Storage.' },
                    { id: 'B', text: 'Use Cloud Scheduler to schedule jobs at a regular interval to check object timestamps.' },
                    { id: 'C', text: '**Configure a Cloud Storage trigger to send a message to Pub/Sub. Use a Pub/Sub-triggered Cloud Function to start the training job on a GKE cluster.**' },
                    { id: 'D', text: 'Configure a Dataflow pipeline to save the files in Cloud Storage and then start the training job.' }
                ],
                correct: 'C',
                explanation: "This is the standard event-driven architecture: **Cloud Storage Event -> Pub/Sub -> Triggered Cloud Function** (or Cloud Run service) to execute the pipeline run."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è",
                question: "You need to train an XGBoost model with custom dependencies via Vertex AI custom training, aiming to **minimize startup time**.",
                options: [
                    { id: 'A', text: 'Create a custom container that includes the data and the custom dependencies.' },
                    { id: 'B', text: '**Store the data in a Cloud Storage bucket, and use the XGBoost prebuilt custom container to run your training application. Create a Python source distribution that installs the custom dependencies at runtime.**' },
                    { id: 'C', text: 'Use the XGBoost prebuilt custom container. Create a Python source distribution that includes the data and installs the custom dependencies at runtime.' }
                ],
                correct: 'B',
                explanation: "To **minimize startup time**, use the **prebuilt container** (avoiding a lengthy build). Dependencies are installed efficiently at runtime via a **Python source distribution** (`--package-path`)."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è",
                question: "What is the best way to develop and debug complex models in TensorFlow while maintaining **ease of debugging** and reducing **training time**?",
                options: [
                    { id: 'A', text: 'Configure a v3-8 TPU VM and SSH into the VM to train and debug the model.' },
                    { id: 'B', text: 'Configure a v3-8 TPU node and use Cloud Shell to SSH into the Host VM.' },
                    { id: 'C', text: 'Configure 4 NVIDIA P100 GPUs and use the ParameterServerStrategy.' },
                    { id: 'D', text: 'Configure **4 NVIDIA P100 GPUs** and use **MultiWorkerMirroredStrategy**.' }
                ],
                correct: 'D',
                explanation: "**MultiWorkerMirroredStrategy** on GPUs is a scalable distributed strategy that is highly compatible with standard Python debugging tools, balancing parallel performance with development flexibility."
            },
            {
                category: "MLOps - CI/CD, Pipelines & Automation ‚öôÔ∏è",
                question: "You have trained a scikit-learn model and want to deploy it to production for online and batch prediction, minimizing extra code.",
                options: [
                    { id: 'A', text: 'Upload your model to the Vertex AI Model Registry using a prebuilt container. Deploy to Endpoints and use `instanceConfig` for batch transformation.' },
                    { id: 'B', text: 'Wrap your model in a **Custom Prediction Routine (CPR)**, build a container image, upload to Model Registry, and deploy for online/batch prediction.' },
                    { id: 'C', text: 'Create a custom container, define a custom serving function, upload both, and deploy for online prediction only.' },
                    { id: 'D', text: 'Create a custom container, upload it, and deploy for batch prediction using `instanceConfig`.' }
                ],
                correct: 'B',
                explanation: "The **Custom Prediction Routine (CPR)** method is designed to package custom code (like scikit-learn models or preprocessing logic) for deployment onto Vertex AI, serving both online and batch predictions with minimal custom infrastructure."
            },
            
            // --- Category 5: MLOps - Serving, Monitoring & Management üí° (Q1, Q14, Q46, Q74, Q163, Q174, Q190, Q325, Q336) ---
            {
                category: "MLOps - Serving, Monitoring & Management üí°",
                question: "You need a serverless pipeline for high-throughput online inference with **computationally expensive preprocessing**.",
                options: [
                    { id: 'A', text: 'Send requests to Pub/Sub. Use Cloud Function for preprocessing. Submit prediction request to AI Platform.' },
                    { id: 'B', text: 'Stream requests into Cloud Spanner. Query a view for preprocessing logic. Submit prediction request to AI Platform.' },
                    { id: 'C', text: 'Send requests to Pub/Sub. **Transform the data using a Dataflow job**. Submit a prediction request to AI Platform using the transformed data.' },
                    { id: 'D', text: 'Deploy two models: one trained on raw data and one trained on preprocessed data.' }
                ],
                correct: 'C',
                explanation: "**Dataflow** is the best managed service for scalable, stateful stream processing and is required to handle the computationally expensive preprocessing before serving the prediction."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí°",
                question: "A DNN model's accuracy is steadily deteriorating due to **data drift**. What is the best immediate step?",
                options: [
                    { id: 'A', text: 'Perform feature selection and retrain the model with fewer features.' },
                    { id: 'B', text: 'Retrain the model using L2 regularization.' },
                    { id: 'C', text: 'Create **alerts to monitor for skew** using Vertex AI Model Monitoring, and **retrain the model**.' },
                    { id: 'D', text: 'Perform feature selection and retrain the model on a monthly basis.' }
                ],
                correct: 'C',
                explanation: "The standard MLOps response to data drift is to deploy **Vertex AI Model Monitoring** to detect the skew/drift and trigger the **retraining pipeline** automatically based on the alert."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí°",
                question: "You need to execute a **batch prediction** on 100 million records in a BigQuery table using a custom TensorFlow model, minimizing effort.",
                options: [
                    { id: 'A', text: 'Run a batch prediction job on Vertex AI that points to the BigQuery table.' },
                    { id: 'B', text: 'Create a Dataflow pipeline to read the data and write the predictions to BigQuery.' },
                    { id: 'C', text: '**Import the TensorFlow model with BigQuery ML, and run the ML.PREDICT function.**' },
                    { id: 'D', text: 'Export data to Cloud Storage and configure a Vertex AI batch prediction job.' }
                ],
                correct: 'C',
                explanation: "By importing the TensorFlow model into **BigQuery ML**, you can run inference directly using simple SQL (`ML.PREDICT`), making it the lowest effort and most scalable approach for batch prediction on BigQuery data."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí°",
                question: "Your DNN model has $10 \text{ ms}$ latency, but requirements demand $8 \text{ ms}$. Accepting a small performance drop, what is the fastest optimization to try?",
                options: [
                    { id: 'A', text: 'Switch from CPU to GPU serving.' },
                    { id: 'B', text: 'Increase the dropout rate to 0.8 and retrain your model.' },
                    { id: 'C', text: 'Apply **quantization** to your SavedModel by reducing the floating point precision to $tf.float16$.' },
                    { id: 'D', text: 'Increase the max\_batch\_size serving parameter.' }
                ],
                correct: 'C',
                explanation: "**Quantization** is a post-training optimization that significantly reduces the model size and latency, achieving fast performance gains without requiring a lengthy retraining cycle."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí°",
                question: "Predictions are slow because the serving pipeline joins real-time cart data with customers' historic purchase data in **BigQuery**. How should you speed up predictions?",
                options: [
                    { id: 'A', text: 'Attach an NVIDIA P100 GPU to your deployed model‚Äôs instance.' },
                    { id: 'B', text: 'Deploy your model to more instances behind a load balancer.' },
                    { id: 'C', text: 'Use a **low latency database** (like Vertex AI Feature Store or Cloud Bigtable) for the customers‚Äô historic purchase behavior.' },
                    { id: 'D', text: 'Create a materialized view in BigQuery with the necessary data for predictions.' }
                ],
                correct: 'C',
                explanation: "BigQuery is optimized for analytics, not low-latency lookups. The bottleneck is feature retrieval. Migrating the critical features to a **low-latency feature store** or key-value store is the correct fix."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí°",
                question: "To **reduce cost** while continuing to quickly detect drift using Vertex AI Model Monitoring, what should you adjust?",
                options: [
                    { id: 'A', text: 'Replace the monitoring job with a custom SQL script running on BigQuery.' },
                    { id: 'B', text: 'Increase the `monitor_interval` parameter in the ScheduleConfig of the monitoring job.' },
                    { id: 'C', text: 'Decrease the **`sample_rate` parameter** in the RandomSampleConfig of the monitoring job.' },
                    { id: 'D', text: 'Use the features and the feature attributions for monitoring.' }
                ],
                correct: 'C',
                explanation: "Monitoring costs are determined by the volume of data analyzed. To reduce cost, you must analyze fewer prediction requests by **decreasing the sampling rate**."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí°",
                question: "A small company needs a **scalable, reliable, cost-effective** serving solution with **minimal infrastructure expertise**.",
                options: [
                    { id: 'A', text: 'Configure Compute Engine VMs to host your models.' },
                    { id: 'B', text: 'Create a Cloud Run function to deploy your models as serverless functions.' },
                    { id: 'C', text: 'Create a managed cluster on Google Kubernetes Engine (GKE), and deploy your models as containers.' },
                    { id: 'D', text: '**Deploy your models on Vertex AI endpoints.**' }
                ],
                correct: 'D',
                explanation: "**Vertex AI Endpoints** is the managed service that provides all necessary production features (autoscaling, monitoring, reliability) with zero infrastructure management burden, making it ideal for a team with limited cloud expertise."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí°",
                question: "LLM application has inconsistent **verbosity**. Seek a **scalable** solution to meet user expectations.",
                options: [
                    { id: 'A', text: 'Implement a **keyword-based routing layer**. If the user\'s input contains keywords, **re-prompt the language model** to summarize or describe.' },
                    { id: 'B', text: 'Ask users to provide examples of responses with the appropriate verbosity as a list of question and answer pairs. Use this dataset to perform supervised fine tuning of the foundational model. Re-evaluate the verb' }
                ],
                correct: 'A',
                explanation: "Controlling LLM behavior (like verbosity) is best done through **prompt engineering** within a routing layer, which is scalable and avoids expensive model retraining."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí°",
                question: "Managing a deployed Vertex AI model and wanting to **automatically retrain** it when performance **deteriorates**.",
                options: [
                    { id: 'A', text: '**Create a Vertex AI Model Monitoring job to track the model\'s performance with production data, and trigger retraining when specific metrics drop below predefined thresholds.**' },
                    { id: 'B', text: 'Collect feedback from end users, and retrain the model based on their assessment of its performance.' },
                    { id: 'C', text: 'Configure a scheduled job to evaluate the model\'s performance on a static dataset, and retrain the model if the performance drops below predefined thresholds.' },
                    { id: 'D', text: 'Use Vertex Explainable AI to analyze feature attributions and identify potential biases in the model. Retrain when significant shifts in feature importance or biases are detected.' }
                ],
                correct: 'A',
                explanation: "The solution must be **automatic**. **Vertex AI Model Monitoring** is the native service that tracks performance against baselines and sends alerts that trigger an automated retraining pipeline when deterioration is detected."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí°",
                question: "Deploying a **custom ML model** for **online serving** to optimize for **high throughput** and minimal **latency** using the simplest solution.",
                options: [
                    { id: 'A', text: 'Deploy the model to a Vertex AI endpoint resource to automatically scale the serving backend based on the throughput. Configure the endpoint\'s autoscaling settings to minimize latency.' },
                    { id: 'B', text: '**Implement a containerized serving solution using Cloud Run. Configure the concurrency settings to handle multiple requests simultaneously.**' },
                    { id: 'C', text: 'Apply simplification techniques such as model pruning and quantization to reduce the model\'s size and complexity. Retrain the model using Vertex AI to improve its performance, latency, memory, and throughput.' },
                    { id: 'D', text: 'Enable request-response logging for the model hosted in Vertex AI. Use Looker Studio to analyze the logs, identify bottlenecks, and optimize the' }
                ],
                correct: 'B',
                explanation: "**Cloud Run** is a simple, serverless solution for custom containers. Its core mechanism for achieving high throughput is optimizing **concurrency settings** to allow a single instance to handle multiple requests in parallel."
            },
            {
                category: "MLOps - Serving, Monitoring & Management üí°",
                question: "A **SQL analyst** needs to utilize a TensorFlow model stored in Cloud Storage using the **simplest and most efficient** approach.",
                options: [
                    { id: 'A', text: '**Import the model into Vertex AI Model Registry**. **Deploy the model to a Vertex AI endpoint**, and use **SQL for inference in BigQuery.**' },
                    { id: 'B', text: 'Deploy the model by using TensorFlow Serving, and call for inference from BigQuery.' },
                    { id: 'C', text: 'Convert the model into a BigQuery ML model, and use SQL for inference.' },
                    { id: 'D', text: 'Import the model into BigQuery, and use SQL for inference.' }
                ],
                correct: 'A',
                explanation: "The solution must enable **SQL inference** for the analyst. This is achieved by deploying the model to a **Vertex AI Endpoint** and leveraging BigQuery's built-in capability to call the endpoint directly using the `ML.PREDICT` SQL function."
            },
        ];
        // --- End Question Data ---

        function loadQuestion(index) {
            if (index < 0 || index >= questions.length) return;
            
            currentQuestionIndex = index;
            isAnswerRevealed = false;
            const q = questions[index];

            document.getElementById('question-counter').textContent = `Question ${index + 1} of ${questions.length}`;
            document.getElementById('question-category-label').textContent = `Category: ${q.category}`;
            document.getElementById('current-question-text').textContent = q.question;

            const optionsContainer = document.getElementById('options-container');
            optionsContainer.innerHTML = '';
            
            q.options.forEach(option => {
                const optionElement = document.createElement('div');
                optionElement.className = 'option-item';
                optionElement.innerHTML = `
                    <span class="option-label" id="option-label-${option.id}">${option.id}.</span>
                    <p class="text-gray-800 text-base flex-grow" id="option-text-${option.id}">${option.text}</p>
                `;
                optionElement.onclick = () => selectOption(option.id);
                optionsContainer.appendChild(optionElement);
            });

            document.getElementById('answer-section').classList.add('hidden');
            document.getElementById('reveal-button').classList.remove('hidden');
            
            updateNavigationButtons();
        }

        function selectOption(selectedId) {
            if (isAnswerRevealed) return;

            questions[currentQuestionIndex].options.forEach(option => {
                const element = document.getElementById(`option-label-${option.id}`).parentNode;
                element.classList.remove('bg-blue-200', 'border-blue-600');
                if (option.id === selectedId) {
                    element.classList.add('bg-blue-200', 'border-blue-600');
                }
            });
        }

        function revealAnswer() {
            isAnswerRevealed = true;
            const q = questions[currentQuestionIndex];
            
            // Mark the correct option in the list
            document.getElementById(`option-label-${q.correct}`).parentNode.classList.remove('bg-gray-50', 'bg-gray-100', 'bg-blue-200');
            document.getElementById(`option-label-${q.correct}`).parentNode.classList.add('bg-green-100', 'border-green-600', 'shadow-lg');
            document.getElementById(`option-label-${q.correct}`).classList.remove('border-gray-400');
            document.getElementById(`option-label-${q.correct}`).classList.add('bg-green-600', 'text-white');

            // Populate and show the answer section
            document.getElementById('correct-choice').textContent = `Choice ${q.correct}`;
            document.getElementById('explanation-text').textContent = q.explanation;
            document.getElementById('answer-section').classList.remove('hidden');
            document.getElementById('reveal-button').classList.add('hidden');
        }

        function navigateQuestion(direction) {
            // If moving forward and the answer hasn't been revealed, prompt the user.
            if (direction === 1 && currentQuestionIndex < questions.length - 1 && !isAnswerRevealed) {
                if (!confirm("Are you sure you want to move to the next question without revealing the answer? This is active learning!")) {
                    return;
                }
            }
            loadQuestion(currentQuestionIndex + direction);
        }

        function updateNavigationButtons() {
            document.getElementById('prev-button').disabled = currentQuestionIndex === 0;
            document.getElementById('next-button').disabled = currentQuestionIndex === questions.length - 1;
        }

        // Initialize the app
        window.onload = () => loadQuestion(0);
    </script>
</body>
</html>
